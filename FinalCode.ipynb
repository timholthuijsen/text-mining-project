{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "complex-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "import re\n",
    "from re import search\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from itertools import permutations\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-microphone",
   "metadata": {},
   "source": [
    "## Preperations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identified-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatabases() -> list:\n",
    "    \"\"\"\n",
    "    Get the list of available datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    LIST\n",
    "        Datasets in the /data/* directory.\n",
    "    \"\"\"\n",
    "    return glob.glob(\"data/*.exl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifth-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFile(filename: str) -> list:\n",
    "    \"\"\"\n",
    "    Cleans the data from empty lines and metadata.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Path to the dataset file.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Cleaned list of lines from the dataset.\n",
    "    \"\"\"\n",
    "    cleaned_text = []\n",
    "    regex = re.compile(r'[<>]')\n",
    "    for line in open(filename, \"r\"):\n",
    "        if not regex.search(line) and not line.startswith(\"STEP\") and len(line) > 10:\n",
    "            cleaned_text.append(line)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subjective-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFiles(filenames: list, pb: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Apply cleanFile on a list of files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filenames : list\n",
    "        List of dataset directories.\n",
    "    pb : bool, optional\n",
    "        Show progress bar of finished files.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of list of strings as cleaned dataset lines.\n",
    "    \"\"\"\n",
    "    all_cleaned_text = []\n",
    "    for i, filename in enumerate(filenames):\n",
    "        cleaned_text = cleanFile(filenames[i])\n",
    "        all_cleaned_text.append(cleaned_text)\n",
    "        if pb: print(f\"Files done: {i+1}/{len(filenames)}\")\n",
    "    if pb: print(\"DONE!\")\n",
    "    return all_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "endless-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(data: list, filename: str = \"temp\") -> None:\n",
    "    \"\"\"\n",
    "    Quick dump some data to a temporary file\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        Data generated from other functions.\n",
    "    filename : str, optional\n",
    "        Name of the temporary file. The default value is \"temp\"\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decimal-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpRead(filename: str = \"temp\") -> list:\n",
    "    \"\"\"\n",
    "    Quick read dumped data\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str, optional\n",
    "        Name of the file to read. The default value is \"temp\"\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Previously saved data by using dump().\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "undefined-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(data: list, n_lines: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Print the first elements of a given dataset\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        Dataset.\n",
    "    n_lines : int, optional\n",
    "        Number of lines to display. The default is 5.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        DESCRIPTION.\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(data):\n",
    "        if i < n_lines:\n",
    "            print(f\"{i}\\t{line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "great-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatFile(data: list) -> list:\n",
    "    \"\"\"\n",
    "    Format a pos-tagged database.\n",
    "    Remove whitespace and punctuation from end and start of words.\n",
    "    Get all words lowercase\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list\n",
    "        Database to format.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Formatted dataset.\n",
    "    \"\"\"\n",
    "    cleaned_db = []\n",
    "    for sentence in data:\n",
    "        cleaned_sentence = []\n",
    "        for word, tag in sentence:\n",
    "            cleaned_sentence.append((word.strip(string.punctuation + \" \").lower(), tag))\n",
    "        cleaned_db.append(cleaned_sentence)\n",
    "    return cleaned_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marked-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(csv: str = 'data/nyt-ingredients-snapshot-2015.csv') -> list:\n",
    "    \"\"\"\n",
    "    Read a given CSV file and save rows as separate dictionaries\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str\n",
    "        Directory to csv file\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of rows as dictionaries\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv)\n",
    "    keys = data.columns\n",
    "    values = data.values\n",
    "    rows = []\n",
    "    for value in values:\n",
    "        d = {}\n",
    "        for i, key in enumerate(keys):\n",
    "            d[key] = value[i]\n",
    "        rows.append(d)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-musician",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "determined-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_plurals(low: list) -> list:\n",
    "    return list(set([singular(word) for word in low]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "young-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos-tagging \n",
    "\n",
    "MAP = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "def pos_tag_db(db: list) -> list:\n",
    "    \"\"\"\n",
    "    PoS-Tag a given database\n",
    "    Parameters\n",
    "    ----------\n",
    "    db : list\n",
    "        Database to PoS-Tag.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        PoS-Tagged database.\n",
    "    \"\"\"\n",
    "    return [nltk.pos_tag(sentence.split(), tagset = \"universal\") for sentence in db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "million-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing \n",
    "\n",
    "def lemmatize_db(db: list, exclude: list = []) -> list:\n",
    "    \"\"\"\n",
    "    Lemmatize given database\n",
    "    Parameters\n",
    "    ----------\n",
    "    db : list\n",
    "        PoS-Tagged dataset to lemmatize.\n",
    "    exclude : list, optional\n",
    "        Types to exclude from the output. The default is [].\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Lemmatized database.\n",
    "    \"\"\"\n",
    "    lemmatized_db = []\n",
    "    for sentence in db:\n",
    "        lemmatized_sentence = []\n",
    "        for w, p in sentence:\n",
    "            if p in exclude:\n",
    "                continue\n",
    "            elif p in MAP.keys():\n",
    "                lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = MAP[p])\n",
    "            else:\n",
    "                lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "            lemmatized_sentence.append((lemma, p))\n",
    "        lemmatized_db.append(lemmatized_sentence)\n",
    "    return lemmatized_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rubber-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES USE:\n",
    "    \n",
    "# # Get a list of available databases\n",
    "# dbs = getDatabases()\n",
    "\n",
    "# # Clean the first one\n",
    "# clean_db = cleanFile(dbs[0])\n",
    "\n",
    "# # Print first 10 lines of database\n",
    "# head(clean_db, 10)\n",
    "\n",
    "# # PoS-Tag, lemmatize and format\n",
    "# postagged = pos_tag_db(clean_db)\n",
    "# lemmatized = lemmatize_db(postagged, [\".\", \"X\"])\n",
    "# formatted = formatFile(lemmatized)\n",
    "\n",
    "# # Print 5 lines from the result\n",
    "# head(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "civic-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting ingredients\n",
    "\n",
    "def list_ingredients():\n",
    "    # read json files\n",
    "    df1 = pd.read_json(\"data/train.json\")\n",
    "    df2 = pd.read_json(\"data/test.json\")\n",
    "    \n",
    "    # extract ingredients coloumn and convert to one list of ingredients\n",
    "    df1_ingre = df1[\"ingredients\"]\n",
    "    df2_ingre = df2[\"ingredients\"]\n",
    "    \n",
    "    all_ingre = pd.Series.tolist(df1_ingre) + pd.Series.tolist(df2_ingre)\n",
    "    \n",
    "    # convert list of lists to a flat list\n",
    "    list_of_ingre = []\n",
    "    \n",
    "    for element in all_ingre:\n",
    "        for item in element:\n",
    "            list_of_ingre.append(item)\n",
    "    \n",
    "    # remove duplicates        \n",
    "    final_ingre = list(dict.fromkeys(list_of_ingre)) # outputs a list of 7137 ingredients\n",
    "    \n",
    "    with open('final_ingr.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow(final_ingre)\n",
    "    \n",
    "    return final_ingre       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "elder-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of ingredients\n",
    "\n",
    "url = \"https://world.openfoodfacts.org/ingredients.json\"\n",
    "page = urlopen(url)\n",
    "html = page.read()\n",
    "soup = BeautifulSoup(html) \n",
    "u = soup.decode('utf-8')\n",
    "u = u.split('\"name\":')\n",
    "\n",
    "#creating a csv file with ingredients\n",
    "localFile = open('ingredients.csv', 'w')\n",
    "\n",
    "#creating a list of ingredients\n",
    "ingredients = []\n",
    "for line in u:\n",
    "    ingredients.append(line.split(',')[0])\n",
    "del ingredients[0]\n",
    "for ingredient in ingredients:\n",
    "    localFile.write(ingredient)\n",
    "localFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adapted-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in ingredients:\n",
    "    # remove any string that contains digits, these are the E numbers\n",
    "    if any(map(str.isdigit, element)):\n",
    "        ingredients.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tight-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_str = str(ingredients)\n",
    "cleanString1 = re.sub('\"','', ingredients_str )\n",
    "cleanString = re.sub(\"'\",'', cleanString1)\n",
    "ingredients = cleanString\n",
    "ingredients = ingredients.split(',')\n",
    "\n",
    "for ingr in ingredients:\n",
    "    if len(ingr) < 4:\n",
    "        ingredients.remove(ingr)\n",
    "final = []   \n",
    "\n",
    "for ingr in ingredients:\n",
    "    final.append(ingr[1:])\n",
    "ingredients = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "crazy-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = [\"egg\", \"fat\", \"oat\", \"ham\", \"tea\", \"ham\", \"oil\",\"pea\", \"rye\", \"fig\", \"cod\", \"ice\"]\n",
    "to_remove = [ 'green', 't', 'cone', 'glaze','ngredients','gredients', 'powder', 'sauce','Sauce' , 'serving', \n",
    "             'whole', 'brown','ingredient', 'flakes', 'maple', 'baking', 'serv', 'black', 'diced', 'white', 'paste',\n",
    "             'roll', 'cooked', 'blend', 'approx', 'cooked', 'ingredients', 'dehydrated']\n",
    "for ingr in ingredients:\n",
    "    if ingr in to_remove:\n",
    "        ingredients.remove(ingr)\n",
    "    if len(ingr) == 3 and ingr.lower() not in saved:\n",
    "        ingredients.remove(ingr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "welcome-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First database : 'VegetarianRecipes.exl'\n",
    "\n",
    "# data structure of clean_db is a list of strings\n",
    "clean_db = cleanFile(getDatabases()[0])\n",
    "  \n",
    "# merging the cleaned sentenes \n",
    "vegetarian_recipes = []\n",
    "\n",
    "for sentence in clean_db:\n",
    "    vegetarian_recipes.append(sentence)\n",
    "    \n",
    "# split into separate recipes\n",
    "split_vegetarian_recipes = str(vegetarian_recipes).split('EACH')\n",
    "del split_vegetarian_recipes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caring-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the recipes to a csv file\n",
    "\n",
    "with open('recipes_veg.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"EACH PORTION\", 'TEMPERATURE', \"METHODstr\", \"METHOD\", \"NOTE\", \"CONTAINS (NOUNS)\", \"INGREDIENTS\"])\n",
    "    # loop through each recipes \n",
    "    allmethod = []\n",
    "    for recipe in split_vegetarian_recipes:\n",
    "        # for each recipe create a list\n",
    "        portion = []\n",
    "        method = []\n",
    "        temperature = []\n",
    "        pan_size = []\n",
    "        note = []\n",
    "        contains = []\n",
    "        for sent in re.split(',|\\n', recipe):\n",
    "            if search('PORTION:', sent):\n",
    "                sent.replace('PORTION:','')\n",
    "                sent.replace('PAN','')\n",
    "                portion.append(sent.replace('PORTION:',''))\n",
    "                    \n",
    "            elif search('TEMPERATURE:', sent):\n",
    "                temperature.append(sent.replace('TEMPERATURE:',''))\n",
    "                \n",
    "            elif search('PAN SIZE:', sent):\n",
    "                pan_size.append(sent.replace('PAN SIZE:',''))\n",
    "                    \n",
    "            elif search('NOTE:', sent):\n",
    "                note.append(sent.replace('NOTE:',''))\n",
    "            else:\n",
    "                method.append(sent)\n",
    "        # merge method (as a list) into one single string\n",
    "        method_str = \"\"\n",
    "        for text in method:\n",
    "            method_str += str(text) + \" \"\n",
    "        savemethodstr = method_str\n",
    "        \n",
    "        for ingr in ingredients:\n",
    "            if ingr.lower() in savemethodstr:\n",
    "                # replace with ingredient mask\n",
    "                savemethodstr = savemethodstr.replace(ingr.lower(), \" INGREDIENT \")\n",
    "                contains.append(ingr.lower())\n",
    "                # turn ingredient into one token\n",
    "                ingr_dash = ingr.lower().replace(' ', '')\n",
    "                method_str = method_str.replace(ingr.lower(), ingr_dash)\n",
    "                \n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        method_str = tokenizer.tokenize(method_str)\n",
    "        # lemmatize and POS tag the method string\n",
    "        # PoS-Tag, lemmatize and format\n",
    "        \n",
    "        postagged = pos_tag_db(method_str)\n",
    "        # lemmatized = lemmatize_db(postagged, [\".\", \"X\"])\n",
    "        formatted = formatFile(postagged)\n",
    "        # method_tagged = formatted\n",
    "        method_tagged = postagged\n",
    "        \n",
    "        \n",
    "        onelist = []\n",
    "        twolist = []\n",
    "        contains_nospace = []\n",
    "        for con in contains:\n",
    "            cons = con.lower().replace(' ', '')\n",
    "            contains_nospace.append(cons)\n",
    "            \n",
    "        for pair in method_tagged:\n",
    "            onelist.extend(pair)\n",
    "            \n",
    "        for w, tag in onelist:\n",
    "            pair = tuple([w, tag])\n",
    "            if w in contains_nospace:\n",
    "                pair2 = tuple((w, 'INGREDIENT'))\n",
    "                twolist.append(pair2) \n",
    "            else:\n",
    "                twolist.append(pair)\n",
    "                \n",
    "                        \n",
    "        # the ingredients are turned into a set, so there is no repetition of ingredients.\n",
    "        writer.writerow([portion, temperature, savemethodstr ,twolist, note, contains, set(contains)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "foreign-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second database: data/ArmedForcesRecipes.exl'\n",
    "\n",
    "# data structure of clean_db is a list of strings\n",
    "clean_db = cleanFile(getDatabases()[2])\n",
    "\n",
    "# merging the cleaned sentenes \n",
    "armedforces_recipes = []\n",
    "for sentence in clean_db:\n",
    "    armedforces_recipes.append(sentence)\n",
    "    \n",
    "# split the recipes into lists\n",
    "split_armedforces_recipes = str(armedforces_recipes).split('EACH')\n",
    "del split_armedforces_recipes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "electoral-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the recipes to a csv file\n",
    "with open('recipes_armed.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"EACH PORTION\", 'TEMPERATURE', \"METHODstr\", \"METHOD\", \"NOTE\", \"CONTAINS (NOUNS)\", \"INGREDIENTS\"])\n",
    "    # loop through each recipes \n",
    "    allmethod = []\n",
    "    for recipe in split_armedforces_recipes:\n",
    "        # for each recipe create a list\n",
    "        portion = []\n",
    "        method = []\n",
    "        temperature = []\n",
    "        pan_size = []\n",
    "        note = []\n",
    "        contains = []\n",
    "        for sent in re.split(',|\\n', recipe):\n",
    "            if search('PORTION:', sent):\n",
    "                sent.replace('PORTION:','')\n",
    "                sent.replace('PAN','')\n",
    "                portion.append(sent.replace('PORTION:',''))\n",
    "                    \n",
    "            elif search('TEMPERATURE:', sent):\n",
    "                temperature.append(sent.replace('TEMPERATURE:',''))\n",
    "                \n",
    "            elif search('PAN SIZE:', sent):\n",
    "                pan_size.append(sent.replace('PAN SIZE:',''))\n",
    "                    \n",
    "            elif search('NOTE:', sent):\n",
    "                note.append(sent.replace('NOTE:',''))\n",
    "            else:\n",
    "                method.append(sent)\n",
    "        # merge method (as a list) into one single string\n",
    "        method_str = \"\"\n",
    "        for text in method:\n",
    "            method_str += str(text) + \" \"\n",
    "        savemethodstr = method_str\n",
    "        \n",
    "        for ingr in ingredients:\n",
    "            if ingr.lower() in savemethodstr:\n",
    "                # replace with ingredient mask\n",
    "                savemethodstr = savemethodstr.replace(ingr.lower(), \" INGREDIENT \")\n",
    "                contains.append(ingr.lower())\n",
    "                # turn ingredient into one token\n",
    "                ingr_dash = ingr.lower().replace(' ', '')\n",
    "                method_str = method_str.replace(ingr.lower(), ingr_dash)\n",
    "                \n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        method_str = tokenizer.tokenize(method_str)\n",
    "        # lemmatize and POS tag the method string\n",
    "        # PoS-Tag, lemmatize and format\n",
    "      \n",
    "        postagged = pos_tag_db(method_str)\n",
    "        # lemmatized = lemmatize_db(postagged, [\".\", \"X\"])\n",
    "        formatted = formatFile(postagged)\n",
    "        # method_tagged = formatted\n",
    "        method_tagged = postagged\n",
    "        \n",
    "        onelist = []\n",
    "        twolist = []\n",
    "        contains_nospace = []\n",
    "        for con in contains:\n",
    "            cons = con.lower().replace(' ', '')\n",
    "            contains_nospace.append(cons)\n",
    "            \n",
    "        for pair in method_tagged:\n",
    "            onelist.extend(pair)\n",
    "            \n",
    "        for w, tag in onelist:\n",
    "            pair = tuple([w, tag])\n",
    "            if w in contains_nospace:\n",
    "                pair2 = tuple((w, 'INGREDIENT'))\n",
    "                twolist.append(pair2) \n",
    "            else:\n",
    "                twolist.append(pair)\n",
    "        # the ingredients are turned into a set, so there is no repetition of ingredients.\n",
    "        writer.writerow([portion, temperature, savemethodstr ,twolist, note, contains, set(contains)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aquatic-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third database: 'data/CommonRecipes.exl'\n",
    "\n",
    "# data structure of clean_db is a list of strings\n",
    "clean_db = cleanFile(getDatabases()[3])\n",
    "\n",
    "# merging the cleaned sentenes \n",
    "common_recipes = []\n",
    "for sentence in clean_db:\n",
    "    common_recipes.append(sentence)\n",
    "    \n",
    "# split the recipes into lists\n",
    "split_common_recipes = str(common_recipes).split('EACH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "jewish-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the recipes to a csv file\n",
    "with open('recipes_common.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"EACH PORTION\", 'TEMPERATURE', \"METHODstr\", \"METHOD\", \"NOTE\", \"CONTAINS (NOUNS)\", \"INGREDIENTS\"])\n",
    "    # loop through each recipes \n",
    "    allmethod = []\n",
    "    for recipe in split_common_recipes:\n",
    "        # for each recipe create a list\n",
    "        portion = []\n",
    "        method = []\n",
    "        temperature = []\n",
    "        pan_size = []\n",
    "        note = []\n",
    "        contains = []\n",
    "        for sent in re.split(',|\\n', recipe):\n",
    "            if search('PORTION:', sent):\n",
    "                sent.replace('PORTION:','')\n",
    "                sent.replace('PAN','')\n",
    "                portion.append(sent.replace('PORTION:',''))\n",
    "                    \n",
    "            elif search('TEMPERATURE:', sent):\n",
    "                temperature.append(sent.replace('TEMPERATURE:',''))\n",
    "                \n",
    "            elif search('PAN SIZE:', sent):\n",
    "                pan_size.append(sent.replace('PAN SIZE:',''))\n",
    "                    \n",
    "            elif search('NOTE:', sent):\n",
    "                note.append(sent.replace('NOTE:',''))\n",
    "            else:\n",
    "                method.append(sent)\n",
    "        #merge method (as a list) into one single string\n",
    "        method_str = \"\"\n",
    "        for text in method:\n",
    "            method_str += str(text) + \" \"\n",
    "        savemethodstr = method_str\n",
    "        \n",
    "        for ingr in ingredients:\n",
    "            if ingr.lower() in savemethodstr:\n",
    "                # replace with ingredient mask\n",
    "                savemethodstr = savemethodstr.replace(ingr.lower(), \" INGREDIENT \")\n",
    "                contains.append(ingr.lower())\n",
    "                # turn ingredient into one token\n",
    "                ingr_dash = ingr.lower().replace(' ', '')\n",
    "                method_str = method_str.replace(ingr.lower(), ingr_dash)\n",
    "                \n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        method_str = tokenizer.tokenize(method_str)\n",
    "        # lemmatize and POS tag the method string\n",
    "        # PoS-Tag, lemmatize and format\n",
    "        \n",
    "    \n",
    "        postagged = pos_tag_db(method_str)\n",
    "        # lemmatized = lemmatize_db(postagged, [\".\", \"X\"])\n",
    "        formatted = formatFile(postagged)\n",
    "        # method_tagged = formatted\n",
    "        method_tagged = postagged\n",
    "        \n",
    "        \n",
    "        onelist = []\n",
    "        twolist = []\n",
    "        contains_nospace = []\n",
    "        for con in contains:\n",
    "            cons = con.lower().replace(' ', '')\n",
    "            contains_nospace.append(cons)\n",
    "            \n",
    "        for pair in method_tagged:\n",
    "            onelist.extend(pair)\n",
    "            \n",
    "        for w, tag in onelist:\n",
    "            pair = tuple([w, tag])\n",
    "            if w in contains_nospace:\n",
    "                pair2 = tuple((w, 'INGREDIENT'))\n",
    "                twolist.append(pair2) \n",
    "            else:\n",
    "                twolist.append(pair)\n",
    "                \n",
    "            \n",
    "        # the ingredients are turned into a set, so there is no repetition of ingredients.\n",
    "        writer.writerow([portion, temperature, savemethodstr ,twolist, note, contains, set(contains)])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-bleeding",
   "metadata": {},
   "source": [
    "## Co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "documented-sleeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'salt', 'size', 'butter', 'margarine', 'syrup...\n",
       "1                                                set()\n",
       "2               {'bacon', 'ice', 'tea', 'rain', 'fat'}\n",
       "3             {'bacon', 'ice', 'rain', 'juice', 'fat'}\n",
       "4                                       {'ice', 'fat'}\n",
       "Name: INGREDIENTS, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 1 - vegetarian\n",
    "df = pd.read_csv('recipes_veg.csv')\n",
    "saved_veg = df['INGREDIENTS'] \n",
    "\n",
    "# dataset 2 - armed\n",
    "df = pd.read_csv('recipes_armed.csv')\n",
    "saved_armed = df['INGREDIENTS'] \n",
    "\n",
    "# dataset 3 - common\n",
    "df = pd.read_csv('recipes_common.csv')\n",
    "saved_common = df['INGREDIENTS'] \n",
    "\n",
    "# creating one dataframe with the set of ingredients\n",
    "frames = [saved_veg, saved_armed, saved_common]\n",
    "result = pd.concat(frames)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "interior-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrences of ingredients\n",
    "\n",
    "cooccs_surface = Counter()\n",
    "\n",
    "# contextual coocurrence\n",
    "for row in result:\n",
    "    for i,w in enumerate(row):\n",
    "        row = row.replace('}', \"\")\n",
    "        row = row.replace('{', \"\")\n",
    "        for w in row.split(','):\n",
    "            for cw in row.split(','):\n",
    "                if cw != w:\n",
    "                    cooccs_surface[(w, cw)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "numerous-retirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\" 'oil'\", \" 'water'\"), 56607),\n",
       " ((\" 'water'\", \" 'oil'\"), 56607),\n",
       " ((\" 'rain'\", \" 'water'\"), 46310),\n",
       " ((\" 'water'\", \" 'rain'\"), 46310),\n",
       " ((\" 'onion'\", \" 'water'\"), 38585),\n",
       " ((\" 'water'\", \" 'onion'\"), 38585),\n",
       " ((\" 'rain'\", \" 'oil'\"), 38546),\n",
       " ((\" 'oil'\", \" 'rain'\"), 38546),\n",
       " ((\" 'onion'\", \" 'oil'\"), 38264),\n",
       " ((\" 'oil'\", \" 'onion'\"), 38264),\n",
       " ((\" 'ice'\", \" 'water'\"), 37995),\n",
       " ((\" 'water'\", \" 'ice'\"), 37995),\n",
       " ((\" 'flour'\", \" 'water'\"), 36606),\n",
       " ((\" 'water'\", \" 'flour'\"), 36606),\n",
       " ((\" 'water'\", \" 'sugar'\"), 36578),\n",
       " ((\" 'sugar'\", \" 'water'\"), 36578),\n",
       " ((\" 'oil'\", \" 'ice'\"), 34546),\n",
       " ((\" 'ice'\", \" 'oil'\"), 34546),\n",
       " ((\" 'onion'\", \" 'ice'\"), 32445),\n",
       " ((\" 'ice'\", \" 'onion'\"), 32445),\n",
       " ((\" 'pepper'\", \" 'water'\"), 31684),\n",
       " ((\" 'water'\", \" 'pepper'\"), 31684),\n",
       " ((\" 'water'\", \" 'salt'\"), 31256),\n",
       " ((\" 'salt'\", \" 'water'\"), 31256),\n",
       " ((\" 'ice'\", \" 'rain'\"), 30580),\n",
       " ((\" 'rain'\", \" 'ice'\"), 30580),\n",
       " ((\" 'water'\", \" 'shortening'\"), 30254),\n",
       " ((\" 'shortening'\", \" 'water'\"), 30254),\n",
       " ((\" 'pepper'\", \" 'oil'\"), 30020),\n",
       " ((\" 'oil'\", \" 'pepper'\"), 30020),\n",
       " ((\" 'onion'\", \" 'rain'\"), 29004),\n",
       " ((\" 'rain'\", \" 'onion'\"), 29004),\n",
       " ((\" 'pepper'\", \" 'onion'\"), 28568),\n",
       " ((\" 'onion'\", \" 'pepper'\"), 28568),\n",
       " ((\" 'tea'\", \" 'water'\"), 28444),\n",
       " ((\" 'water'\", \" 'tea'\"), 28444),\n",
       " ((\" 'cons'\", \" 'water'\"), 28275),\n",
       " ((\" 'water'\", \" 'cons'\"), 28275),\n",
       " ((\" 'flour'\", \" 'oil'\"), 28032),\n",
       " ((\" 'oil'\", \" 'flour'\"), 28032),\n",
       " ((\" 'oil'\", \" 'sugar'\"), 27070),\n",
       " ((\" 'sugar'\", \" 'oil'\"), 27070),\n",
       " ((\" 'water'\", \" 'milk'\"), 27023),\n",
       " ((\" 'milk'\", \" 'water'\"), 27023),\n",
       " ((\" 'salt'\", \" 'oil'\"), 25927),\n",
       " ((\" 'oil'\", \" 'salt'\"), 25927),\n",
       " ((\" 'cons'\", \" 'oil'\"), 25483),\n",
       " ((\" 'oil'\", \" 'cons'\"), 25483),\n",
       " ((\" 'flour'\", \" 'sugar'\"), 25443),\n",
       " ((\" 'sugar'\", \" 'flour'\"), 25443)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccs_surface.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "second-cycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [('Score', 'NOUN'), ('cored', 'VERB'), ('unpee...\n",
       "1                                                   []\n",
       "2    [('Arrange', 'NOUN'), ('slices', 'NOUN'), ('in...\n",
       "3    [('3', 'NUM'), ('In', 'ADP'), ('Step', 'NOUN')...\n",
       "4    [('2', 'NUM'), ('GRILLED', 'NOUN'), ('BACON', ...\n",
       "Name: METHOD, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 1 - vegetarian\n",
    "df = pd.read_csv('recipes_veg.csv')\n",
    "saved_veg = df['METHOD'] \n",
    "\n",
    "# dataset 2 - armed\n",
    "df = pd.read_csv('recipes_armed.csv')\n",
    "saved_armed = df['METHOD'] \n",
    "\n",
    "# dataset 3 - common\n",
    "df = pd.read_csv('recipes_common.csv')\n",
    "saved_common = df['METHOD'] \n",
    "\n",
    "# creating one dataframe with the methods\n",
    "frames = [saved_veg, saved_armed, saved_common]\n",
    "method = pd.concat(frames)\n",
    "method.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "tropical-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrences of verbs vs ingredients\n",
    "\n",
    "verb_cooccs_surface = Counter()\n",
    "\n",
    "spansize = 5\n",
    "for row in method:\n",
    "    row = row.split(\"),\")\n",
    "    for i,w in enumerate(row):\n",
    "        w = w.replace('(', \"\")\n",
    "        w = w.replace('[', \"\")\n",
    "        if \"VERB\" in w:\n",
    "            span_range = list(range(max(i - spansize, 0), i)) # left side indices (range, then list so we can extend)\n",
    "            span_range.extend(range(i + 1, min(i + spansize + 1, len(w)))) # extend by right side indices\n",
    "            for cw in [row[idx] for idx in span_range]:\n",
    "                if 'INGREDIENT'in cw:\n",
    "                    verb_cooccs_surface[(w.lower(), cw)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "departmental-description",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\" 'add', 'verb'\", \" ('water', 'INGREDIENT'\"), 55),\n",
       " ((\" 'greased', 'verb'\", \" ('batter', 'INGREDIENT'\"), 38),\n",
       " ((\" 'melted', 'verb'\", \" ('salad', 'INGREDIENT'\"), 34),\n",
       " ((\" 'melted', 'verb'\", \" ('oil', 'INGREDIENT'\"), 33),\n",
       " ((\" 'may', 'verb'\", \" ('garlic', 'INGREDIENT'\"), 31),\n",
       " ((\" 'be', 'verb'\", \" ('garlic', 'INGREDIENT'\"), 31),\n",
       " ((\" 'used', 'verb'\", \" ('garlic', 'INGREDIENT'\"), 30),\n",
       " ((\" 'add', 'verb'\", \" ('milk', 'INGREDIENT'\"), 27),\n",
       " ((\" 'floured', 'verb'\", \" ('batter', 'INGREDIENT'\"), 22),\n",
       " ((\" 'used', 'verb'\", \" ('water', 'INGREDIENT'\"), 20),\n",
       " ((\" 'will', 'verb'\", \" ('celery', 'INGREDIENT'\"), 20),\n",
       " ((\" 'running', 'verb'\", \" ('chicken', 'INGREDIENT'\"), 20),\n",
       " ((\" 'boiling', 'verb'\", \" ('water', 'INGREDIENT'\"), 19),\n",
       " ((\" 'running', 'verb'\", \" ('water', 'INGREDIENT'\"), 19),\n",
       " ((\" 'be', 'verb'\", \" ('chicken', 'INGREDIENT'\"), 19),\n",
       " ((\" 'may', 'verb'\", \" ('juice', 'INGREDIENT'\"), 17),\n",
       " ((\" 'be', 'verb'\", \" ('juice', 'INGREDIENT'\"), 17),\n",
       " ((\" 'used', 'verb'\", \" ('juice', 'INGREDIENT'\"), 17),\n",
       " ((\" 'may', 'verb'\", \" ('water', 'INGREDIENT'\"), 17),\n",
       " ((\" 'be', 'verb'\", \" ('water', 'INGREDIENT'\"), 17),\n",
       " ((\" 'make', 'verb'\", \" ('water', 'INGREDIENT'\"), 17),\n",
       " ((\" 'may', 'verb'\", \" ('tomato', 'INGREDIENT'\"), 17),\n",
       " ((\" 'be', 'verb'\", \" ('tomato', 'INGREDIENT'\"), 17),\n",
       " ((\" 'used', 'verb'\", \" ('tomato', 'INGREDIENT'\"), 17),\n",
       " ((\" 'may', 'verb'\", \" ('concentrate', 'INGREDIENT'\"), 17),\n",
       " ((\" 'bring', 'verb'\", \" ('water', 'INGREDIENT'\"), 17),\n",
       " ((\" 'may', 'verb'\", \" ('chicken', 'INGREDIENT'\"), 17),\n",
       " ((\" 'is', 'verb'\", \" ('flour', 'INGREDIENT'\"), 16),\n",
       " ((\" 'be', 'verb'\", \" ('concentrate', 'INGREDIENT'\"), 16),\n",
       " ((\" 'used', 'verb'\", \" ('concentrate', 'INGREDIENT'\"), 16),\n",
       " ((\" 'stirring', 'verb'\", \" ('water', 'INGREDIENT'\"), 16),\n",
       " ((\" 'melted', 'verb'\", \" ('shortening', 'INGREDIENT'\"), 16),\n",
       " ((\" 'quartered', 'verb'\", \" ('chicken', 'INGREDIENT'\"), 16),\n",
       " ((\" 'is', 'verb'\", \" ('dough', 'INGREDIENT'\"), 15),\n",
       " ((\" 'is', 'verb'\", \" ('cheese', 'INGREDIENT'\"), 15),\n",
       " ((\" 'add', 'verb'\", \" ('fat', 'INGREDIENT'\"), 13),\n",
       " ((\" 'add', 'verb'\", \" ('flour', 'INGREDIENT'\"), 13),\n",
       " ((\" 'add', 'verb'\", \" ('liquid', 'INGREDIENT'\"), 12),\n",
       " ((\" 'add', 'verb'\", \" ('salt', 'INGREDIENT'\"), 11),\n",
       " ((\" 'removing', 'verb'\", \" ('beans', 'INGREDIENT'\"), 11),\n",
       " ((\" 'salted', 'verb'\", \" ('water', 'INGREDIENT'\"), 11),\n",
       " ((\" 'add', 'verb'\", \" ('vanilla', 'INGREDIENT'\"), 11),\n",
       " ((\" 'remove', 'verb'\", \" ('water', 'INGREDIENT'\"), 11),\n",
       " ((\" 'incorporated', 'verb'\", \" ('flour', 'INGREDIENT'\"), 10),\n",
       " ((\" 'make', 'verb'\", \" ('liquid', 'INGREDIENT'\"), 10),\n",
       " ((\" 'add', 'verb'\", \" ('garlic', 'INGREDIENT'\"), 10),\n",
       " ((\" 'may', 'verb'\", \" ('beans', 'INGREDIENT'\"), 10),\n",
       " ((\" 'bring', 'verb'\", \" ('salt', 'INGREDIENT'\"), 10),\n",
       " ((\" 'add', 'verb'\", \" ('pepper', 'INGREDIENT'\"), 10),\n",
       " ((\" 'stirring', 'verb'\", \" ('beef', 'INGREDIENT'\"), 10)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_cooccs_surface.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-pillow",
   "metadata": {},
   "source": [
    "## LSTM RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "minor-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM MODEL CODE IS ADAPTED FROM:\n",
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "united-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure \n",
    "\n",
    "DATANAME = \"final_shortened\"   # Name of the processed data file\n",
    "N_RECIPES = 5                  # Number of sentences to generate when generate() is called\n",
    "N_EPOCHS = 20                  # Number of epochs in model training\n",
    "BATCH_SIZE = 128               # Bar-tch size while model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "indoor-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(p=True):\n",
    "    \"\"\"\n",
    "    Read the existing data\n",
    "    p: bool = Be verbose\n",
    "    Outputs a tuple of names, units and recipes\n",
    "    \"\"\"\n",
    "    if p: print(\"Preparing data...\")\n",
    "    data = pd.read_csv(\"data/nyt-ingredients-snapshot-2015.csv\")\n",
    "    names = [str(name).lower().strip(string.punctuation + \" \") for name in data[\"name\"]] + list_ingredients()\n",
    "    units = {str(unit).lower().strip(string.punctuation + \" \") for unit in data[\"unit\"] if len(str(unit)) < 14 \\\n",
    "             and not str(unit)[0].isdigit()}\n",
    "    databases = getDatabases()\n",
    "    recipes = []\n",
    "    for i in range(len(databases)):\n",
    "        recipes += [row for row in cleanFile(databases[i]) if len(row) > 50 and not row[0].isdigit()]\n",
    "        if p: print(f\"Loaded database {i+1}/{len(databases)}\")\n",
    "    if p: print(\"Prepared data!\\n\")\n",
    "    return (names, units, recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alike-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, dumpname=DATANAME):\n",
    "    \"\"\"\n",
    "    Clear and format the data\n",
    "    \"\"\"\n",
    "    print(\"Processing data...\")\n",
    "    names, units, recipes = data\n",
    "    db = []\n",
    "    len_recipes = len(recipes)\n",
    "    for i, row in enumerate(recipes):\n",
    "        if i%100 == 0: print(f\"Processing data... {i}/{len_recipes}\")\n",
    "        for word in row.split(\" \"):\n",
    "            word = word.strip().strip(string.punctuation + \" \")\n",
    "            if len(word) > 0:\n",
    "                if word in names or check_contains(word, names):\n",
    "                    db.append(\"INGREDIENT\")\n",
    "                else:\n",
    "                    db.append(word.lower())\n",
    "        db.append(\"\\n\")\n",
    "    dump(db, DATANAME)\n",
    "    print(f\"Processed data! Saved as {DATANAME}\\n\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "beautiful-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars():\n",
    "    \"\"\"\n",
    "    Prepare the data for training\n",
    "    \"\"\"\n",
    "    data = dumpRead(DATANAME)\n",
    "    words = sorted(list(set(data)))\n",
    "    ewords = dict((c, i) for i, c in enumerate(words))\n",
    "    enumbs = dict((i, c) for i, c in enumerate(words))\n",
    "    seq_length = 100\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(data) - seq_length, 1):\n",
    "     \tseq_in = data[i:i + seq_length]\n",
    "     \tseq_out = data[i + seq_length]\n",
    "     \tdataX.append([ewords[char] for char in seq_in])\n",
    "     \tdataY.append(ewords[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "    X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "    X = X / float(len(words))\n",
    "    y = np_utils.to_categorical(dataY)\n",
    "    return (X, y, dataX, dataY, words, ewords, enumbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "latin-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(var, epochs=N_EPOCHS, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create the LSTM model and save it as a .hdf5 file\n",
    "    REQUIRES A FOLDER NAMED \"model\" IN THE DIRECTORY\n",
    "    TAKES VERY LONG TO RUN\n",
    "    \"\"\"\n",
    "    X, y, dataX, dataY, words, ewords, enumbs = var\n",
    "    print(\"Creating model...\")\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    filepath=\"model/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n",
    "    print(\"Created model!\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fitted-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recipe(var, fname = \"weights-20-3.1225.hdf5\"):\n",
    "    \"\"\"\n",
    "    Use the created model at \"model/{fname}\" to generate text\n",
    "    Returns a recipe\n",
    "    \"\"\"\n",
    "    X, y, dataX, dataY, words, ewords, enumbs = var\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    filename = \"model/\" + fname\n",
    "    model.load_weights(filename)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    start = np.random.randint(0, len(dataX)-1)\n",
    "    pattern = dataX[start]\n",
    "    recipe = []\n",
    "    for i in range(50):\n",
    "        if i == 0 or recipe[-1] != \"\\n\":\n",
    "            result = \"\"\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "            x = x / float(len(words))\n",
    "            prediction = model.predict(x, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            result = enumbs[index]\n",
    "            recipe.append(result)\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "    # print(recipe)\n",
    "    return recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hawaiian-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_recipe(var):\n",
    "    \"\"\"\n",
    "    Uses create_recipe() to create a recipe\n",
    "    Then selects the better ones\n",
    "    \"\"\"\n",
    "    recipe = [\"INGREDIENT\"]\n",
    "    def count_ingredients():\n",
    "        n = 0\n",
    "        for word in recipe:\n",
    "            if \"INGREDIENT\" in word.upper():\n",
    "                n += 1\n",
    "        return n/len(recipe)\n",
    "    attempts = 0\n",
    "    # Make sure the sentence is decently long, and ingredients make up less than 40% of the sentence.\n",
    "    while len(recipe) < 8 or count_ingredients() > 0.4:\n",
    "        recipe = create_recipe(var=var)\n",
    "        attempts += 1\n",
    "        # Quit if recipe cannot be generated\n",
    "        if attempts > 99:\n",
    "            recipe = [\"ERROR:\", \"\", \"\", \"\", \"Failed\", \"to\", \"generate\", \"a\", \"recipe\", \"\\n\"]\n",
    "    # Remove common words that cannot end the sentence\n",
    "    while recipe[-2] in [\"to\", \"from\", \"is\", \"until\", \"and\", \"when\", \"an\", \"a\", \"or\", \"if\", \"on\", \"at\"]:\n",
    "        recipe = recipe[:-2] + recipe[-1:]\n",
    "    # Generate ingredients\n",
    "    ingredients = generate_ingredients()\n",
    "    j = -1\n",
    "    units = prepare_data(p=False)[1]\n",
    "    # Put the generated iingredients in place\n",
    "    for i, word in enumerate(recipe):\n",
    "        if word == \"INGREDIENT\":\n",
    "            j = min(j+1, len(ingredients)-1)\n",
    "            recipe[i] = ingredients[j]\n",
    "        elif word == \"UNIT\":\n",
    "            recipe[i] = random.choice(list(units))\n",
    "    return \".\".join(\" \".join(recipe).split(\" \\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "excited-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contains(i, l, p=False):\n",
    "    \"\"\"\n",
    "    Check if the item i (or singular versions) are in the list l\n",
    "    \"\"\"\n",
    "    if p or len(i) > 3:\n",
    "        return (i[-1] == \"s\" and i[:-1] in l) or (i[-2:] == \"es\" and i[:-2] in l) or (i[-3:] == \"ies\" and i[:-3] + \"y\" in l)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "essential-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_coocs():\n",
    "    \"\"\"\n",
    "    Read the file \"Cococcs of ingredients\"\n",
    "    \"\"\"\n",
    "    raw = dict(dumpRead(\"data/Cococcs of ingredients\"))\n",
    "    coocs = dict()\n",
    "    for key in raw:\n",
    "        coocs[(key[0][2:-1].lower(), key[1][2:-1].lower())] = raw[key]\n",
    "    return dict(sorted(coocs.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "least-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ingredients_h(il=[], init=random.choice(dumpRead(\"model/inglist_final\"))):\n",
    "    \"\"\"\n",
    "    Helper function for generate_ingredients()\n",
    "    \"\"\"\n",
    "    coocs = read_coocs()\n",
    "    selected = il\n",
    "    for key in coocs:\n",
    "        if len(il) < 20 and key[0] == init and key[0] not in selected:\n",
    "            selected.append(key[1])\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "proper-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ingredients():\n",
    "    \"\"\"\n",
    "    Generate a list of related ingredients\n",
    "    It is possible to comment out this code and add a list of ingredients manually.\n",
    "    For example:\n",
    "        selected = [\"pasta\", \"tomato\", \"garlic\", \"oil\", \"onions\", \"salt\", \"pepper\"]\n",
    "        return selected\n",
    "    This way you can use custom ingredients\n",
    "    \"\"\"\n",
    "    first = \"\"\n",
    "    selected = []\n",
    "    # Choose a random, valid first ingredient\n",
    "    while first == \"\" or len(first) > 30:\n",
    "        first = random.choice(dumpRead(\"model/inglist_final\"))\n",
    "    # Get a list of 20 ingredients that are related to the previous ones\n",
    "    # Not all 20 are used, this is just to make sure that the model does not run out of ingredients.\n",
    "    # 20 is deduced by:\n",
    "    #   [maximum length of a sentence] x [maximum percentage of ingredients in a sentence]\n",
    "    #   50 x 0.40 = 20\n",
    "    while len(selected) < 20:\n",
    "        selected += generate_ingredients_h(init=random.choice(dumpRead(\"model/inglist_final\")))\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-israel",
   "metadata": {},
   "source": [
    "## Run the model \n",
    "\n",
    "HOW TO USE\n",
    "1. `prepare()`\n",
    "2. `create()`\n",
    "3. Update `create_recipe()` to use the desired .hdf5 file\n",
    "4. `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "palestinian-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data (skip if it is already processed)\n",
    "def prepare(fname=DATANAME):\n",
    "    raw_data = prepare_data()\n",
    "    process_data(raw_data, fname)\n",
    "\n",
    "# Create an LSTM model for text generation\n",
    "def create(epochs=N_EPOCHS, batch_size=BATCH_SIZE):\n",
    "    var = init_vars()\n",
    "    create_model(var=var, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Generate n sentences for the recipe\n",
    "def generate(n=N_RECIPES):\n",
    "    var = init_vars()\n",
    "    for i in range(n):\n",
    "        print(f\"{i+1}/{n}:\", select_recipe(var))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
